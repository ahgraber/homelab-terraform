{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Homelab infrastructure managment with Ansible and Terraform","text":"<p>This repo helps manage homelab infra with pxe, ansible, and/or terraform</p> <p>With inspiration from the k8s-at-home community, especially onedr0p's cluster template</p>"},{"location":"#overview","title":"Overview","text":"<ul> <li>\ud83d\udcdd Prerequisites</li> <li>\ud83d\udce1 Provision with Terraform</li> <li>\ud83e\uddda Provision with PXE</li> <li>\ud83e\udd16 Manage with Ansible</li> <li>Infra</li> <li>TrueNAS SCALE</li> <li>UPS</li> <li>Notes</li> <li>Crowdsec</li> <li>Format and Mount Drives</li> <li>Bootable Ubuntu USB</li> </ul>"},{"location":"#repository-structure","title":"\ud83d\udcc2 Repository structure","text":"<pre><code>ansible\n\u251c\u2500\u2500 inventory - ansible inventory definitions, group_vars, and host_vars\n\u251c\u2500\u2500 playbooks\n\u2514\u2500\u2500 roles\nmain.tf - terraform definitions\n</code></pre>"},{"location":"1-prerequisites/","title":"Prerequitites &amp; Preparation","text":""},{"location":"1-prerequisites/#direnv","title":"direnv","text":"<p>It is advisable to install direnv to persist environmental variables to a hidden <code>.envrc</code> file.</p> <p>After direnv is installed, set up on the local repository path:</p> <pre><code># add direnv hooks\necho 'eval \"$(direnv hook zsh)\"' &gt;&gt; ~/.zshrc\nsource ~/.zshrc\n\n# add .envrc and .env to gitignores (global, local)\ngit config --global core.excludesFile '~/.gitignore'\ntouch ~/.gitignore\necho '.envrc' &gt;&gt; ~/.gitignore\necho '.env' &gt;&gt; ~/.gitignore\necho '.envrc' &gt;&gt; .gitignore\necho '.env' &gt;&gt; .gitignore\n\n# remove .gitignored files\ngit ls-files -i --exclude-from=.gitignore | xargs git rm --cached\n\n# set up direnv config to whitelist folders for direnv\nmkdir -p ~/.config/direnv\ncat &gt; ~/.config/direnv/direnv.toml &lt;&lt; EOF\n[whitelist]\nprefix = [ \"/path/to/folders/to/whitelist\" ]\nexact = [ \"/path/to/envrc/to/whitelist\" ]\nEOF\n\ndirenv reload\n</code></pre>"},{"location":"1-prerequisites/#govmomigovc","title":"govmomi/govc","text":"<p><code>govc</code> is a vSphere CLI built on top of govmomi.</p> <p>The CLI is designed to be a user friendly CLI alternative to the GUI and well suited for automation tasks. It also acts as a test harness for the govmomi APIs and provides working examples of how to use the APIs.</p> <p>We will use it to identify names of vSphere resources</p> <pre><code># these variables should be known from VCSA installation\ncat &lt;&lt; EOF &gt;&gt; .envrc\nexport GOVC_URL=\"vsphere-ip-or-hostname\"\nexport GOVC_USERNAME=\"administrator@example.com\"\nexport GOVC_PASSWORD=\"changeme\"\nexport GOVC_DATACENTER=Homelab\nexport GOVC_INSECURE=true\nEOF\n</code></pre>"},{"location":"1-prerequisites/#using-govc","title":"Using govc","text":"<p>See docs for usage</p> <pre><code># find networks\ngovc find -type n\n# find resource pool path\ngovc find -type p\n# find datastore\ngovc find -type s\n</code></pre>"},{"location":"1-prerequisites/#ansible","title":"Ansible","text":"<ol> <li> <p>Install pipx</p> </li> <li> <p>Install ansible</p> </li> </ol> <p><code>sh    pipx install --include-deps ansible    # inject 'docker' and 'kubernetes' pkgs into 'ansible' namespace    pipx inject ansible docker kubernetes</code></p> <ol> <li>Update Ansible requirements</li> </ol> <p><code>sh    ansible-galaxy install -r ./ansible/requirements.yaml --force</code></p> <ol> <li>Update python requirements</li> </ol> <p><code>sh    # pip3 install -r ./ansible/requirements.txt    # use conda env instead :)</code></p>"},{"location":"1-prerequisites/#terraform","title":"Terraform","text":""},{"location":"1-prerequisites/#vm-images","title":"VM Images","text":""},{"location":"2-terraform/","title":"Bootstrap a cluster with Terraform","text":""},{"location":"2-terraform/#customization","title":"Customization","text":"<ol> <li>Add terraform files to <code>.gitignore</code></li> </ol> <p>```sh    cat &lt;&lt; EOF &gt;&gt; .gitignore    .terraform/    .tfstate .lock.hcl    .zip    *.ova    .env    .envrc</p> <p>modules/terraform-vsphere/    archive/</p> <p>main.tf    govcvars.sh    EOF    ```</p> <ol> <li> <p>Review <code>./main.tf.template</code> and module examples    and customize configuration as needed. Remember that we'll substitute secret environmental variables in.</p> </li> <li> <p>Update <code>.envrc</code> with secrets</p> </li> </ol> <p>```sh    # these variables should be known from VCSA installation    cat &lt;&lt; EOF &gt;&gt; .envrc    # vars for govc    export GOVC_URL=\"vsphere-ip-or-hostname\"    export GOVC_USERNAME=\"administrator@example.com\"    export GOVC_PASSWORD=\"changeme\"    export GOVC_DATACENTER=Homelab    export GOVC_INSECURE=true</p> <p># vars for 'main.tf'    export TF_VAR_VSPHERE_USER=\"administrator@example.com\"    export TF_VAR_VSPHERE_USER_PASS=\"changeme\"    export TF_VAR_VSPHERE_SERVER=\"vcenter.example.com\"    export TF_VAR_VSPHERE_DC=\"\"    export TF_VAR_VSPHERE_VMRP=\"Cluster/Resources/poolname\"    export TF_VAR_VSPHERE_VMFOLDER=\"folderna e\"    export TF_VAR_VSPHERE_DATASTORE=\"datastore/dsname\"    export TF_VAR_VSPHERE_VMTEMPLATE=\"ubuntu_2004-k8s-nodhcp\"    export TF_VAR_VSPHERE_PORTGROUP=\"DPortGrp-name\"    export TF_VAR_DNS='[\"10.42.42.1\", \"10.42.42.2\"]'    export TF_VAR_DOMAIN=\"example.com\"    export TF_VAR_GATEWAY=\"10.42.42.1\"</p> <p>export TF_VAR_CTRL_IPs='[\"10.42.42.10\", \"10.42.42.11\", \"10.42.42.12\"]'    export TF_VAR_WORK_IPs='[\"10.42.42.30\", \"10.42.42.31\", \"10.42.42.32\"]'    export TF_VAR_KUBE_VIP=\"10.42.42.42\"</p> <p>export TF_VAR_NODE_USER=\"username\"    export TF_VAR_NODE_PASS=\"changeme\"    export TF_VAR_SSH_ID=\"ssh-rsa IamANsshKey12345== administrator@example.com</p> <p>export TF_VAR_ANSIBLE_HOSTS_FILE=\"./ansible/inventory/cluster/host.ini\"    export TF_VAR_ANSIBLE_PLAYBOOK_DIR=\"./ansible/playbooks\"    export TF_VAR_KUBECONFIG=$(expand_path ./kubeconfig)</p> <p>EOF    ```</p> <ol> <li> <p>Update <code>main.tf.template</code> template file with non-secret info (machine specs, IP addresses. etc)</p> </li> <li> <p>Source environmental variables</p> </li> </ol> <p>```zsh    # reload all env variables    direnv allow .</p> <p>```</p>"},{"location":"2-terraform/#bootstrap","title":"Bootstrap","text":"<p>Run terraform commands</p> <pre><code>terraform init\nterraform plan\nterraform apply  # -auto-approve\n</code></pre> <p>Terraform will</p> <ol> <li>Provision VMs on vSphere</li> <li>Create a hosts.ini file based on inventory.tmpl which is then used by Ansible</li> </ol>"},{"location":"2-terraform/#update","title":"Update","text":"<p>NOTE: Terraform expects it will be used to manage all infrastructure changes. To update currently 'managed' deployment:</p> <ol> <li>Run <code>terraform plan</code> against the updated <code>main.tf</code> file. <code>plan</code> will warn if the change will require destroying/reprovisioning a replacement host</li> <li>Run <code>terraform apply</code> to execute</li> </ol>"},{"location":"2-terraform/#destroy","title":"Destroy","text":"<p>To tear down terraform-managed infra, run:</p> <pre><code>terraform destroy  # -auto-approve\n</code></pre>"},{"location":"3-ansible/","title":"Manage hosts with Ansible","text":"<ul> <li>Manage hosts with Ansible</li> <li>Setup</li> <li>Check Ansible connection</li> <li>Send arbitrary commands</li> <li>Host management</li> <li>k3s install</li> <li>use k3s</li> <li>k3s uninstall</li> </ul>"},{"location":"3-ansible/#setup","title":"Setup","text":"<ol> <li>Ensure .envrc vars are set</li> </ol> <p>```sh    cat &lt;&lt; EOF &gt;&gt; .envrc    # vars for ansible    export KUBECONFIG=$(expand_path ./kubeconfig)    export user=\"...\"    export domain=\"...\"</p> <p># vars for pxe/ubuntu ansible    export gateway=\"10.2.0.1\"    export pxe_server=\"10.2.1.1\"    export default_pass=\"...\"    export crypted_pass='...' # docker run --rm -it alpine:latest mkpasswd -m sha512     export email=\"EXAMPLE@DOMAINCOM&gt;\"    export ssh_rsa=\"ssh-rsa ...\"    export ssh_ed25519=\"ssh-ed25519 ...\" <p># vars for k3s    export kubevip_address=\"10.2.113.1\"    export calico_node_cidr=\"10.2.118.0/24\"    EOF    ```</p> <p><code>sh    direnv allow .</code></p> <ol> <li>Initialize templates</li> </ol> <p>```sh    # reload all env variables    direnv allow .</p> <p>### running from repo root    # create SOPS hook for secret encryption    envsubst &lt; ./templates/ubuntu_vars.yaml.tmpl &gt;! ./ansible/inventory/group_vars/ubuntu/ubuntu_vars.sops.yaml    envsubst &lt; ./templates/k8s_vars.yaml.tmpl &gt;! ./ansible/inventory/group_vars/kubernetes/k8s_vars.sops.yaml</p> <p>export GPG_TTY=$(tty)    # Encrypt SOPS secrets    sops --encrypt --in-place ./ansible/inventory/group_vars/ubuntu/ubuntu_vars.sops.yaml    sops --encrypt --in-place ./ansible/inventory/group_vars/kubernetes/k8s_vars.sops.yaml    ```</p>"},{"location":"3-ansible/#check-ansible-connection","title":"Check Ansible connection","text":"<p>The Terraform build will generate an ansible hosts file</p> <pre><code>### paths assume running from /ansible dir\n# list hosts\nansible all -i ./inventory --list-hosts\n# list groups\nansible-inventory -i ./inventory --graph\n\n# ping hosts\nansible all -i ./inventory --one-line -m 'ping'\nansible all -i ./inventory --one-line -m 'ping' -vvv # for debugging\n</code></pre>"},{"location":"3-ansible/#send-arbitrary-commands","title":"Send arbitrary commands","text":"<pre><code>ansible &lt;groupname&gt; -m ansible.builtin.shell -a \"apt upgrade -y\" --become\n</code></pre>"},{"location":"3-ansible/#host-management","title":"Host management","text":"<pre><code>### paths assume running from /ansible dir\ncd ./ansible/\n\n### install external roles\nansible-galaxy install -r requirements.yaml --force\nansible-galaxy collection install -r requirements.yaml  --force\n\n### assuming we're using 'ubuntu' as group identifier\nansible-playbook -i ./inventory -l ubuntu ./playbooks/ubuntu/reboot.yaml --become\nansible-playbook -i ./inventory -l ubuntu ./playbooks/ubuntu/shutdown.yaml --become\n\n# Ubuntu setup\nansible-playbook -i ./inventory -l ubuntu ./playbooks/ubuntu/os-init.yaml --become --ask-become-pass\n\n# Ubuntu/apt upgrade\nansible-playbook -i ./inventory -l ubuntu ./playbooks/ubuntu/apt-update.yaml\n\n# Crowdsec setup\nansible-playbook -i ./inventory -l crowdsec ./playbooks/crowdsec/cs-install.yaml --become\nansible-playbook -i ./inventory -l crowdsec ./playbooks/crowdsec/cs-update.yaml --become\n\n# Ubuntu OS upgrade\nansible-playbook -i ./inventory -l ubuntu ./playbooks/ubuntu/os-upgrade.yaml\n\n# ...\n\n# Install additional packages on TrueNas Scale\nansible-playbook -i ./inventory -l nas ./playbooks/truenas/packages.yaml --become\n</code></pre>"},{"location":"3-ansible/#k3s-install","title":"k3s install","text":"<pre><code>### paths assume running from ansible/ dir\ncd ./ansible/\n\n### assuming we're using 'kubernetes' as group identifier\n# prep\nansible-playbook -i ./inventory -l kubernetes ./playbooks/kubernetes/k3s-prep.yaml --become # --ask-become-pass\n# install -- this may take 2 runs to complete without error\nansible-playbook -i ./inventory -l kubernetes ./playbooks/kubernetes/k3s-install.yaml --become # --ask-become-pass\n# copy kubeconfig to homelab-gitops-k3s\n\n# rollout-reboot\nansible-playbook -i ./inventory -l kubernetes ./playbooks/kubernetes/k3s-reboot.yaml --become\n# hard reboot\nansible-playbook -i ./inventory -l kubernetes ./playbooks/kubernetes/k3s-hardreboot.yaml --become\n\n# shutdown\nansible-playbook -i ./inventory -l kubernetes ./playbooks/kubernetes/k3s-shutdown.yaml --become\n</code></pre>"},{"location":"3-ansible/#use-k3s","title":"use k3s","text":"<pre><code>kubectl --kubeconfig=${KUBECONFIG} get nodes -o wide\nkubectl --kubeconfig=${KUBECONFIG} get pods -A -o wide\n</code></pre> <p>See homelab-gitops-k3s</p>"},{"location":"3-ansible/#k3s-uninstall","title":"k3s uninstall","text":"<pre><code># uninstall\nansible kubernetes -m ansible.builtin.systemd -a \"name=k3s state=stopped\" --become\nansible-playbook -i ./inventory -l kubernetes ./playbooks/kubernetes/k3s-nuke.yaml --become # --ask-become-pass\n# clean up rook-ceph\nansible-playbook -i ./inventory -l kubernetes ./playbooks/kubernetes/rook-ceph-cleanup.yaml --become # --ask-become-pass\n# hard reboot\nansible-playbook -i ./inventory -l kubernetes ./playbooks/ubuntu/reboot.yaml --become\n</code></pre>"},{"location":"pxe/","title":"PXE Boot","text":"<ul> <li>PXE Boot</li> <li>Overview</li> <li>Prerequisites<ul> <li>Host / Management machine running config via Ansible</li> <li>OPNsense</li> <li>Fileserver</li> <li>Nodes</li> </ul> </li> <li>Create PXE server</li> <li>References<ul> <li>grub.cfg</li> <li>cloud-config</li> </ul> </li> <li>Alternatives</li> </ul>"},{"location":"pxe/#overview","title":"Overview","text":"<ul> <li>Ansible renders the configuration files for each bare metal machine (like IP, hostname...) from templates</li> <li>Ansible syncs the configuration files to OPNsense, which has been configured as PXE server</li> <li>Hosts that are configured for PXE boot will pull the image from OPNsense and netboot/autoinstall</li> </ul>"},{"location":"pxe/#prerequisites","title":"Prerequisites","text":""},{"location":"pxe/#host-management-machine-running-config-via-ansible","title":"Host / Management machine running config via Ansible","text":"<ul> <li>ansible</li> <li>python-netaddr</li> <li>xorriso</li> </ul>"},{"location":"pxe/#opnsense","title":"OPNsense","text":"<p>OPNsense tftp/netboot will provide the grub efi boot file and config to hand off to a fileserver</p> <ul> <li><code>os-tftp</code> package installed</li> <li>DHCP service on appropriate subnet [Services &gt; DHCPv4 &gt; <code>&lt;INTERFACE&gt;</code>] is configured for <code>network boot</code>.   At minimum:</li> </ul> <p><code>txt   Set next-server IP:        192.168.1.1  # the TFTP server, aka our OPNsense device's IP   Set default bios filename: grubx64.efi   # or pxelinux.0 for legacy bios</code></p>"},{"location":"pxe/#fileserver","title":"Fileserver","text":"<p><code>tftp</code> is a poor way to transfer a full OS iso. A local web- or NFS- server is a much better solution to deliver large files to the pxe machine.</p> <p>We will use TrueNAS with webdav http server.</p>"},{"location":"pxe/#nodes","title":"Nodes","text":"<ul> <li>Configure BIOS:</li> <li>disable c-states</li> <li>enable PCIe wake</li> <li>enable wake-on-lan</li> <li>enable boot from network</li> <li>set boot priority for network</li> <li>disable CSM/legacy boot</li> </ul> <p>To re/install OS from PXE, the NIC must have boot priority, otherwise the node will boot from disk Once the OS is PXE-installed, can set priority to local drive</p>"},{"location":"pxe/#create-pxe-server","title":"Create PXE server","text":"<ol> <li> <p>Configure inventory</p> </li> <li> <p>From ansible directory, run with ansible:</p> </li> </ol> <p>```sh    # # install ansible packages    # ansible-galaxy collection install -r requirements.yaml    # compile pxe components, launch server, and boot</p> <p># test render (use localhost password)    ansible-playbook -i ./inventory ./playbooks/pxeboot/build.yaml --tags \"render\" --ask-become-pass</p> <p># copy cloud-config to gist?</p> <p># test push to opnsense    ansible-playbook -i ./inventory ./playbooks/pxeboot/build.yaml --tags \"push\"</p> <p># full send    ansible-playbook -i ./inventory ./playbooks/pxeboot/build.yaml --ask-become-pass    ```</p>"},{"location":"pxe/#references","title":"References","text":"<ul> <li>source - khuedoan</li> <li>ubuntu docs - netboot</li> <li>ubuntu wiki - netboot</li> <li>ubuntu pxe</li> <li>ubuntu pxe 2</li> <li>onedr0p ubuntu pxe   and readme</li> <li>automated install</li> <li>opn as pxe server</li> <li>python package for pxe</li> <li>Boot Ubuntu providing it network config in NoCloud Datasource</li> </ul>"},{"location":"pxe/#grubcfg","title":"grub.cfg","text":"<ul> <li>Jingella grub pxe boot</li> <li>pxe boot with grub</li> <li>UEFI PXE boot with grub</li> </ul>"},{"location":"pxe/#cloud-config","title":"cloud-config","text":"<ul> <li>add cloud-init to iso</li> <li>add cloud-init to iso 2</li> <li>test cloud-init with multipass</li> </ul>"},{"location":"pxe/#alternatives","title":"Alternatives","text":"<p>Ubuntu/Canonical MAAS and MaaS at home Sidero Rackn Digital Rebar Provider and edgelab tinkerbell</p>"},{"location":"infra/opnsense/","title":"OPNsense","text":""},{"location":"infra/opnsense/#vlans","title":"VLANs","text":"<ol> <li>LAN</li> <li>HOME</li> <li>LAB</li> <li>IOT</li> </ol>"},{"location":"infra/opnsense/#dns","title":"DNS","text":"<p>Unbound is used as forwarding DNS resolver. AdGuard Home is used for adblocking on HOME and IOT VLANs.</p>"},{"location":"infra/opnsense/#unbound","title":"Unbound","text":"<p>Critical settings listed below. If a setting is not mentioned, it does not necessarily mean the setting is disabled/ignored; Try with system defaults.</p> Sidebar Setting Value General Enabled [x] General Port 53 General Register DHCP Leases [x] General Register DHCP Static Mappings [x] Advanced Private Domains Access Lists Access Control List RFC1918 alias Query Forwarding Custom Forwarding direct  to  DNS over TLS Custom Forwarding Address: 9.9.9.9 Port: 853 Hostname: dns.quad9.net DNS over TLS Custom Forwarding Address: 149.112.112.112 Port: 853 Hostname: dns.quad9.net DNS over TLS Custom Forwarding Address: 2620:fe::fe Port: 853 Hostname: dns.quad9.net DNS over TLS Custom Forwarding Address: 2620:fe::9 Port: 853 Hostname: dns.quad9.net <p>Configure each VLAN/subnet to use Unbound by setting the subnet IP address as the DNS server in DHCP services configuration</p>"},{"location":"infra/opnsense/#adguard-home","title":"AdGuard Home","text":"<p>Ref</p> <ol> <li>Add <code>mimugmail</code> repo</li> </ol> <p><code>sh    # add repo    fetch -o /usr/local/etc/pkg/repos/mimugmail.conf https://www.routerperformance.net/mimugmail.conf    # update pkg list    pkg update</code></p> <ol> <li> <p>Install <code>os-adguardhome-maxit</code> package from OPNsense GUI: System &gt; Firmware &gt; Packages</p> </li> <li> <p>Enable AdGuard Home service in OPNsense GUI: Services &gt; Adguardhome &gt; General</p> </li> <li> <p>Configure AdGuard Home.  Navigate to :3000 <p>IMPORTANT!  Configure DNS service to be available on port <code>53530</code> (not <code>53</code>) so it does not collide with Unbound!</p> Settings Menu Setting Value DNS Settings Upstream DNS servers 127.0.0.1:53 (OPNsense Unbound) DNS Settings Bootstrap DNS servers 127.0.0.1:53 (OPNsense Unbound) DNS Settings Private reverse DNS servers 127.0.0.1:53 (OPNsense Unbound) 10.2.118.2 (k8s_gateway IP) DNS Settings Use private reverse DNS resolvers [x] DNS Settings Enable reverse resolving of client IP addresses [x] Encryption Settings Enable Encryption [x] Encryption Settings Server Name Encryption Settings Redirect to HTTPS Automatically [x] Encryption Settings HTTPS Port 44353 Encryption Settings DNS-over-TLS Port 853 Encryption Settings DNS-over-QUIC Port 784 Encryption Settings Certificates path /var/etc/acme-client/home//fullchain.cer Encryption Settings Private key path /var/etc/acme-client/home//.key <ol> <li>Add filter lists (see filterlists.com)</li> </ol>"},{"location":"infra/opnsense/#dns-redirects-with-nat","title":"DNS Redirects with NAT","text":"<p>In order to force devices to use AdGuard Home for blocking, we can intercept and redirect DNS queries with NAT port forwarding.</p> Interface Proto Src Address Src Ports Dst Address Dst Ports NAT IP NAT Ports Description No Redirect All Interfaces TCP/UDP This Firewall * * 53 (DNS) * NAT/DNS: Do not redirect DNS for OPNsense Home TCP/UDP ! This Firewall * ! HOSTS_k8s_gateway 53 (DNS) 10.1.0.1 53530 (Adguard Home) NAT/DNS: Redirect DNS to Adguard (HOME) IoT TCP/UDP ! This Firewall * ! HOSTS_k8s_gateway 53 (DNS) 10.3.0.1 53530 (Adguard Home) NAT/DNS: Redirect DNS to Adguard (IoT)"},{"location":"infra/truenas/","title":"TrueNAS SCALE","text":"<ul> <li>TrueNAS SCALE</li> <li>Specs</li> <li>Setup<ul> <li>Prerequisites</li> <li>Networking</li> <li>Storage Pool</li> <li>SMART and SCRUB tasks</li> <li>iSCSI shares</li> <li>NFS shares</li> <li>SMB share / Time Machine volume</li> <li>S3 with Minio</li> <li>Enable WebDav share to host files</li> </ul> </li> <li>Troubleshooting<ul> <li>SMART test controls</li> </ul> </li> <li>QOL Changes</li> <li>Expanding VM Disk<ul> <li>Useful commands</li> </ul> </li> </ul>"},{"location":"infra/truenas/#specs","title":"Specs","text":"<ul> <li>MOBO: Asus Pro WS X570 ACE</li> <li>CPU: Ryzen 5900x</li> <li>RAM: 32 GB</li> <li>BOOT: Samsung 970 EVO 250 GB</li> <li>NIC: Intel X520</li> <li>GPU: NVidia Titan X</li> </ul>"},{"location":"infra/truenas/#setup","title":"Setup","text":""},{"location":"infra/truenas/#prerequisites","title":"Prerequisites","text":"<ol> <li>Create iSCSI VLANs (21, 22) on switch.  Set SFP+ port to TAGGED for LAB, iSCSI_1, iSCSI_2</li> <li>Set up gateway on switch for vlan21 and vlan22</li> <li>Set up default routes on switch for vlan21 and vlan22</li> </ol>"},{"location":"infra/truenas/#networking","title":"Networking","text":"<ol> <li>Set up basic networking for Web console on <code>enp8s0</code>: 10.2.1.1/16</li> <li>Assuming standard 1G Ethernet <code>enp8s0</code> and Intel x520-1 10G NIC <code>enp4s0</code> (ref):</li> <li>define Static Routes:<ul> <li>10.2.0.0/16 to 10.2.0.1 (to OPNsense LAB interface)</li> <li>10.21.21.0/24 to 10.21.21.1 (to iSCSI gateway on 10G switch)</li> <li>10.22.22.0/24 to 10.22.22.1 (to iSCSI gateway on 10G switch)</li> </ul> </li> <li><code>enp8s0</code>: this is the standard interface we created in step 1 above</li> <li><code>enp9s0f1</code>: connect to LAN as pseudo 'iPMI' interface; assign IP in LAN/MGMT subnet (or do not assign in TrueNAS)</li> <li><code>enp4s0</code>: do not assign IP</li> <li>add <code>systemctl restart ix-netif.service</code> as post-init command in Data Protection &gt; Init/Shutdown Scripts so that manual reconfig is not required every reboot (v20.12)</li> <li>create VLAN interfaces:</li> <li>vlan21 for iSCSI_1; assign IP in 10.21.21.0/24</li> <li>vlan22 for iSCSI_2; assign IP in 10.22.22.0/24</li> </ol>"},{"location":"infra/truenas/#storage-pool","title":"Storage Pool","text":"<ol> <li>In console, run <code>geom disk list</code> to see names/IDs of all identified disks</li> <li>Set up zpool</li> <li>(OPTIONAL)Set NVME ssd as L2ARC with <code>zpool add &lt;pool_name&gt; cache /dev/&lt;drive_id&gt;</code></li> <li>Set OPTANE as SLOG with <code>zpool add &lt;pool_name&gt; log /dev/&lt;drive_id&gt;</code></li> <li>First Overprovision drive] with <code>disk_resize &lt;device&gt; &lt;size&gt;</code></li> </ol>"},{"location":"infra/truenas/#smart-and-scrub-tasks","title":"SMART and SCRUB tasks","text":"<ol> <li>Follow instructions</li> </ol>"},{"location":"infra/truenas/#iscsi-shares","title":"iSCSI shares","text":"<ol> <li>Follow wizard:</li> <li>Select appropriate zvol; configure for VMWare</li> <li>For Portal, set IP addresses to addressess assigned to iSCSI vlans</li> <li>For Initiators, set authorized networks to 10.2.0.0/20, 10.21.21.0/24, and 10.22.22.0/24</li> </ol> <p>Refs:</p> <ul> <li>https://www.servethehome.com/building-a-lab-part-3-configuring-vmware-esxi-and-truenas-core/    democratic-csi/README.md</li> </ul>"},{"location":"infra/truenas/#nfs-shares","title":"NFS shares","text":"<ol> <li>Create Dataset in Storage for appropriate pool</li> <li>Create share in Shares &gt; NFS</li> <li>Ensure mapall user and mapall group are <code>root</code></li> <li>Ensure permissions are allowed for internal networks</li> </ol>"},{"location":"infra/truenas/#smb-share-time-machine-volume","title":"SMB share / Time Machine volume","text":"<ol> <li>Create <code>timemachine</code> user and group</li> <li>Create dataset for share</li> <li>Grant \"timemachine\" group full control of timemachine dataset through the ACL editor</li> <li><code>View Permissions</code></li> <li>Update owner to <code>timemachine</code></li> <li><code>Set ACL</code> -&gt; Use ACL Preset <code>POSIX - Restricted</code></li> <li>Create <code>SMB</code> share pointing to <code>timemachine</code> dataset; set as <code>multi-user time machine</code> share</li> <li>Restart SMB server</li> <li>Set quota on Mac for auto pruning:</li> <li>Identify <code>timemachine</code> destination ID: <code>tmutil destinationinfo</code></li> <li>Set quota: <code>sudo tmutil setquota &lt;DESTINATION_ID&gt; &lt;QUOTA_IN_GB&gt;</code></li> </ol> <p>Refs:</p> <ul> <li>https://www.truenas.com/community/threads/multi-user-time-machine-purpose.99276/#post-684995</li> <li>https://www.reddit.com/r/MacOS/comments/lh0yjc/configure_a_truenas_core_share_as_a_time_machine/</li> </ul>"},{"location":"infra/truenas/#s3-with-minio","title":"S3 with Minio","text":"<p>As of <code>TrueNAS-SCALE-22.12.3.1</code>, the integrated Minio/S3 service is deprecated. Instead, iXSystems suggests using the Minio app (which deploys a container via a TrueNAS k8s cluster).</p> <p>Documentation is poor; the community forum has a decent walkthrough, replicated here for reference.</p> <p>If migrating from integrated Minio to Minio app, deploy both to migrate data. Ensure the 'new' Minio uses nonstandard ports so there is no overlap/collision.</p> <ol> <li> <p>Create a new ZFS dataset for minio</p> </li> <li> <p>Using the shell, create 2 directories within that dataset: <code>mkdir -p &lt;/path/to/minio/&gt;{certs,data}</code></p> </li> <li> <p>Create a TrueNAS cron job (System Settings \u2192 Advanced \u2192 Cron Jobs):</p> </li> </ol> Key Value Description Scrutiny Command cp /etc/certificates/le-prod-cert.crt /mnt/ssdpool/minio/certs/public.crt &amp;&amp; cp /etc/certificates/le-prod-cert.key /mnt/ssdpool/minio/certs/private.key &amp;&amp; chmod 444 /mnt/ssdpool/minio/certs/private.key Run As User root Schedule Daily <ol> <li>Create a minio deployment:</li> <li>Apps &gt; search \"Minio\" &gt; Install</li> <li> Key Value Application Name Minio Version (whatever is latest) [1.7.16] Workload Configuration Update Strategy Create new pods and then kill old ones Minio Configuration Enable Distributed Mode <code>disabled</code> [needs 4 instances] Minio Extra Arguments No items have been added Root User Access Key:  [lowercase only] Root Password Security Key:  [alpha-numeric only] Minio Image Environment No items have been added Minio Service Configuration Port default: <code>9000</code> Console Port default: <code>9002</code> Log Search API Configuration Enable Log Search API <code>Disabled</code> [Requires Postgres Database] Storage Minio Data Mount Point Path <code>/export</code> Host Path for Minio Data Volume <code>Enabled</code> Host Path Data Volume <code>&lt;/path/to/minio&gt;/data</code> Extra Host Path Volumes Mount Path in Pod <code>/etc/minio/certs</code> Host Path <code>&lt;path/to/minio&gt;/certs/</code> Postgres Storage Postgres Data Volume <code>Disabled</code> Postgres Backup Volume <code>Disabled</code> Advanced DNS Settings DNS Configuration / DNS Options No items have been added Resource Limits Enable Pod resource limits <code>Disabled</code> <li> <p>Edit the minio deployment to set deployment status probes to use <code>HTTPS</code></p> </li> <p><code>sh    k3s kubectl edit deployment.apps/minio -n ix-minio</code></p> <p>Edit with vi -- use <code>i</code> to enter insert mode, <code>esc</code> to exit, and <code>:wq</code> to save and quit</p> <p><code>yaml    livenessProbe:      failureThreshold: 5      httpGet:        path: /minio/health/live        port: 9001        scheme: HTTP      ...    readinessProbe:      failureThreshold: 5      httpGet:        path: /minio/health/live          port: 9001        scheme: HTTP      ...    startupProbe:      failureThreshold: 60      httpGet:        path: /minio/health/live        port: 9001        scheme: HTTP      ...</code></p> <ol> <li> <p>If replacing built-in Minio service, replicate Minio deployments</p> </li> <li> <p>Sync configurations (if needed) (NOTE: skipped this step)</p> <p><code>sh   mc admin config export &lt;old&gt; &gt; config.txt   # edit as needed   mc admin config import &lt;new&gt; &lt; config.txt</code></p> </li> <li> <p>Compare policies and sync</p> <p><code>sh   mc admin policy list &lt;old&gt;   mc admin policy list &lt;new&gt;   # if replication needed   mc admin policy info &lt;old&gt; &lt;policyname&gt; -f &lt;policyname&gt;.json   mc admin policy add &lt;new&gt; &lt;policyname&gt; &lt;policyname&gt;.json</code></p> </li> <li> <p>Export/add users</p> <p><code>sh   mc admin user list &lt;old&gt;   # if replication needed   mc admin user add &lt;new&gt; &lt;name&gt;  # this will prompt for secret key</code></p> </li> <li> <p>Scale down k8s resources that require minio:</p> <p><code>sh   # suspend   flux suspend hr -n default --all \\     &amp;&amp; kubectl scale deploy -n default --replicas=0 --all \\     &amp;&amp; kubectl annotate cluster postgres -n default --overwrite cnpg.io/hibernation=on   flux suspend hr -n datasci --all \\     &amp;&amp; kubectl scale deploy -n datasci --replicas=0 --all \\     &amp;&amp; kubectl annotate cluster datasci -n datasci --overwrite cnpg.io/hibernation=on   flux suspend hr -n monitoring --all \\     &amp;&amp; kubectl scale deploy -n monitoring --replicas=0 --all   flux suspend hr -n volsync --all \\     &amp;&amp; kubectl scale deploy -n volsync --replicas=0 --all</code></p> </li> <li> <p>Mirror data</p> <p>NOTE: run k8s scale-down (see below) before mirroring!</p> <p><code>sh   mc mirror --preserve &lt;old&gt; &lt;new&gt;</code></p> </li> <li> <p>Stop and disable S3 Service to prevent it from restarting</p> </li> <li> <p>Stop MinIO Application and adjust ports as needed to replace the S3 Service</p> <ol> <li>Adjust in TrueNAS</li> <li>Update <code>mc</code> config.json</li> </ol> </li> <li> <p>Scale deployements back up</p> <p><code>sh   # resume   kubectl annotate cluster postgres -n default cnpg.io/hibernation-   kubectl annotate cluster datasci -n datasci cnpg.io/hibernation-   flux resume hr -n default --all   flux resume hr -n datasci --all   flux resume hr -n monitoring --all   flux resume hr -n volsync --all</code></p> </li> <li> <p>Update prometheusconfig for new minio</p> <p><code>sh   mc admin prometheus generate &lt;new&gt;</code></p> </li> <li> <p>Allow Minio to scrape its own metrics        Set 2 environment variables on the MinIO application configuration:</p> <p>TrueNAS GUI &gt; Apps &gt; Applications &gt; MINIO &gt; Edit</p> <pre><code> Minio Image Environment\n Set the following two environment variables:\n MINIO_PROMETHEUS_URL  --&gt; `https://prometheus.${SECRET_DOMAIN}`\n MINIO_PROMETHEUS_JOB_ID --&gt; job name (`truenas-minio`)\n</code></pre> </li> </ol> <p>Refs:</p> <ul> <li>https://www.truenas.com/community/threads/truenas-scale-s3-service-to-minio-application-migration.110787/</li> <li>https://www.truenas.com/docs/scale/scaletutorials/apps/communityapps/minioclustersscale/minioclustering/</li> <li>https://www.truenas.com/docs/scale/scaletutorials/apps/communityapps/minioclustersscale/miniomanualupdate/</li> </ul>"},{"location":"infra/truenas/#enable-webdav-share-to-host-files","title":"Enable WebDav share to host files","text":"<p>PXEboot server See pxe.md</p> <ol> <li>Create <code>pxeboot</code> dataset</li> <li>Create <code>webdav</code> share for <code>pxeboot</code> dataset (NOTE: TrueNAS now requires WebDAV install as container app)</li> </ol>"},{"location":"infra/truenas/#troubleshooting","title":"Troubleshooting","text":"<p>HD troubleshooting</p>"},{"location":"infra/truenas/#smart-test-controls","title":"SMART test controls","text":"<p>Assuming drive named 'sdb' <code>smartctl -a /dev/sdb</code> (show all smart attributes) <code>smartctl -t short /dev/sdb</code> (perform short smart check) <code>smartctl -t long /dev/sdb</code> (perform long smart check) <code>smartctl -c /dev/sdb</code> (show how long tests would take, not entirely accurate) <code>smartctl -l selftest /dev/sdb</code> (show only test results versus smartctl -a which shows everything) <code>smartctl -X /dev/sdb</code> (stops test in progress.)</p> <p>Hint: if results are too long to scroll, append <code>| more</code> to the end of the command to paginate</p> <p>Here's a loop to keep the drive spun up if you use a USB dock that puts the drive to sleep after a period of time. Use Ctrl-C to break. (not necessarily FreeNAS related)</p> <pre><code>while true; do clear; smartctl -l selftest /dev/sdb; sleep 300; done\n</code></pre> <p>Read multiple smart reports using \"save_smartctl.sh\":</p> <pre><code>#!/bin/bash\n### call script with \"save_smartctl.sh /path/to/outfile\"\n\n# Declare a string array with type\ndeclare -a DiskArray=(\"sda\" \"sdb\" \"sdc\" \"sdd\" \"sde\" \"sdf\" \"sdg\" \"sdh\" \"sdi\" \"skj\" \"sdk\" \"sdl\" \"sdm\")\n\n# Read the array values with space\nfor val in \"${DiskArray[@]}\"; do\n  smartctl -a /dev/${val} &gt;&gt; $1\ndone\n</code></pre>"},{"location":"infra/truenas/#qol-changes","title":"QOL Changes","text":"<ul> <li>Change timeout for session</li> <li>Allow <code>apt</code> install: <code>chmod +x /bin/apt*</code></li> <li>Install Eternal Terminal</li> </ul>"},{"location":"infra/truenas/#expanding-vm-disk","title":"Expanding VM Disk","text":"<p>If you have a Linux VM, which uses the LLVM filesystem, you can easily increase the disk space available to the VM.</p> <p>Linux Logical Volume Manager allows you to have logical volumes (LV) on top of logical volume groups (VG) on top of physical volumes (PV) (ie partitions).</p> <p>This is conceptually similar to zvols on pools on vdevs in zfs.</p> <p>NOTE: These commands may require root or 'sudo' access</p>"},{"location":"infra/truenas/#useful-commands","title":"Useful commands","text":"<pre><code>pvs # list physical volumes\nlvs # list logical volumes\nlvdisplay # logical volume display\npvdisplay # physical volume display\ndf # disk free space\n</code></pre> <ol> <li>Get current status</li> </ol> <p><code>sh    df -h  # get human-readable disks    lvs  # view logical volumes    pvs  # view physical volumes</code></p> <p>In this example, we assume the <code>ubuntu-lv</code> LV is on the <code>ubuntu-vg</code> VG is on the PV <code>/dev/sda3</code> (that's partition 3 of device sda)</p> <ol> <li> <p>Shutdown the VM.</p> </li> <li> <p>Edit the ZVOL in TrueNAS to change the size.</p> </li> <li> <p>Restart the VM.</p> </li> <li> <p>In the VM, run <code>parted</code> with the device ID, repair the GPT information and resize the partition, as per below.</p> </li> </ol> <p>```sh    parted /dev/sda</p> <p>###   in 'parted'   ###    # show partitions    print</p> <p># parted will offer to fix the GPT. Run fix with    f</p> <p># resize the partition (we use 3 because '/dev/sda3')    resizepart 3 100%</p> <pre><code>  # exit 'parted'\n</code></pre> <p>```</p> <ol> <li>Now that the partition table has been resized, we have to resize the physical volume</li> </ol> <p><code>sh    pvdisplay # get current status    pvresize /dev/sda3 # resize    pvdisplay # check work</code></p> <ol> <li>Use 'lvextend' to resize the LV and resize the the filesystem over the resized Physical Volume.</li> </ol> <p><code>sh    lvextend --resizefs ubuntu-vg/ubuntu-lv /dev/sda3</code></p> <ol> <li>Finally... you can check the freespace again.</li> </ol> <p><code>sh    df -h</code></p>"},{"location":"infra/ups/","title":"Uninterruptible Power Supply (UPS)","text":""},{"location":"infra/ups/#ups-setup","title":"UPS setup","text":"<ol> <li>Plug devices into critical / noncritical outlet groups, where noncritical outlets get shut down first</li> </ol>"},{"location":"infra/ups/#opnsense-as-network-ups-tools-nut-server","title":"OPNsense as Network UPS Tools (NUT) Server","text":"<p>Refs:</p> <ul> <li>https://schnerring.net/blog/configure-nut-for-opnsense-and-truenas-with-the-cyberpower-pr750ert2u-ups/</li> <li>https://forum.opnsense.org/index.php?topic=27936.0</li> </ul> <ol> <li> <p>Install NUT plugin</p> </li> <li> <p>Restart</p> </li> <li> <p>Configure NUT</p> </li> <li>General Settings &gt; General Nut Settings<ul> <li>Name will be used as address for all netclients</li> </ul> </li> <li>General Settings &gt; Nut Account Settings<ul> <li>Monitor Password is password for <code>monuser</code> account that will be used for all netclients</li> </ul> </li> <li> <p>UPS Type &gt; USBHID-Driver</p> <ul> <li>[x] enable</li> </ul> </li> <li> <p>Configure NAT: port forward internal traffic hitting firewall IPs port <code>3493</code> to <code>127.0.0.1:3493</code></p> </li> <li> <p>Test</p> </li> </ol> <p><code>sh    upsc &lt;UPS_NAME&gt;@&lt;OPNSENSE_IP&gt;:3493</code></p>"},{"location":"infra/ups/#truenas-integration","title":"TrueNAS integration","text":"<ol> <li> <p>Ensure NUT configured on OPNsense (acting as NUT server)</p> </li> <li> <p>On TrueNAS, test with:</p> </li> </ol> <p><code>sh    upsc &lt;UPS_NAME&gt;@&lt;OPNSENSE_IP&gt;:3493</code></p> <ol> <li> <p>Find configuration: System Settings &gt; Services &gt; UPS</p> </li> <li> <p></p> </li> <li> <p>Test that it actually shuts devices down:</p> </li> </ol> <p><code>sh    # from server (OPNsense)    upsmon -c fsd</code></p>"},{"location":"infra/ups/#sample-configurations","title":"Sample configurations","text":""},{"location":"infra/ups/#nutconf","title":"nut.conf","text":"<pre><code>MODE=netclient\n</code></pre>"},{"location":"infra/ups/#upsmonconf","title":"upsmon.conf","text":"<pre><code>MONITOR PR1500RT2U@&lt;opnsense_address&gt;:3493 1 monuser &lt;password&gt; slave\n# bsd\n; SHUTDOWNCMD \"/usr/local/etc/rc.halt\"\n# linux\nSHUTDOWNCMD /sbin/shutdown -h +0\nPOWERDOWNFLAG /etc/killpower\n</code></pre>"},{"location":"notes/crowdsec/","title":"Crowdsec","text":"<p>Crowdsec is an open-source, lightweight agent to detect and respond to bad behaviors. It also automatically benefits from our global community-wide IP reputation database</p> <p>Crowdsec is composed of an <code>agent</code> that parses logs and creates alerts, and a <code>local API (LAPI)</code> that transforms these alerts into decisions. The agent and the LAPI can run on the same node/container or separately (the <code>agent</code> must be present to take action). In complex configurations, it makes sense to have <code>agents</code> on each machine that runs the protected applications, and a single LAPI that gathers all signals from agents and communicates with the central API.</p>"},{"location":"notes/crowdsec/#cscli-crowdsec-terminal-interface","title":"<code>cscli</code>: crowdsec terminal interface","text":"<p>installation instructions</p> <ul> <li>compile and install natively</li> <li>run in container</li> </ul>"},{"location":"notes/crowdsec/#opnsense","title":"OPNsense","text":"<p>Crowdsec offers an OPNsense plugin ref.</p> <p>By installing the CrowdSec plugin, available through the OPNsense repositories, you can:</p> <ul> <li>use the OPNsense server as LAPI for other agents and bouncers</li> <li>deploy an agent on OPNsense and scan its logs for attacks</li> <li>block attackers from your whole network with a single firewall bouncer</li> <li>list the hub plugins (parsers, scenarios..) and decisions on the OPNsense admin interface</li> </ul>"},{"location":"notes/crowdsec/#opnsense-config","title":"OPNsense config","text":"<p>In general, the default installation configuration is just fine on OPNsense.</p> <ul> <li>if using OPNsense deployment as central LAPI, set LAPI listen address in <code>Services &gt; CrowdSec &gt; Settings</code></li> <li>it may make sense to whitelist internal IP addresses:</li> </ul> <p><code>txt   # /path/to/crowdsec/parsers/s02-enrich/whitelist.yaml   name: admin-whitelist   description: \"Whitelist events from private/admin ipv4 addresses\"   whitelist:     reason: \"private ipv4/ipv6 ip/ranges\"     ip:       - \"127.0.0.1\"       -  \"::1\"       - \"&lt;opnsense ips&gt;\"       - \"&lt;network infra IP&gt;\"       - \"&lt;admin static IP&gt;\"</code></p>"},{"location":"notes/crowdsec/#multi-server","title":"Multi-server","text":"<ul> <li>The agent is in charge of processing the logs, matching them against scenarios,   and sending the resulting alerts to the local API</li> <li>The local API (LAPI from now on) receives the alerts and converts them into decisions based on your profile</li> <li>The bouncer(s) query the LAPI to receive the decisions to be applied</li> </ul>"},{"location":"notes/format_and_mount/","title":"Format and mount new disk","text":"<p>Ref: https://techguides.yt/guides/how-to-partition-format-and-auto-mount-disk-on-ubuntu-20-04/</p>"},{"location":"notes/format_and_mount/#steps","title":"Steps","text":"<ol> <li>Get disk identifiers</li> </ol> <p><code>sh    sudo fdisk -l</code></p> <ol> <li>Wipe target disk</li> </ol> <p><code>sh    # enter 'gdisk' for disk '/dev/sda'    sudo gdisk /dev/sda    # choose delete operation    d  # delete    1  # partition 1    d  # continue    w  # write / execute the operation to the disk</code></p> <ol> <li>Create new partition table</li> </ol> <p><code>sh    # enter 'gdisk' for disk '/dev/sda'    sudo gdisk /dev/sda    n # create new partition    1 # partition 1    &lt;enter&gt; &lt;enter&gt; &lt;enter&gt; # accept defaults    w # write/execute operation to disk</code></p> <ol> <li>Format the disk</li> </ol> <p><code>sh    sudo mkfs.ext4 /dev/sda1</code></p> <ol> <li>Get disk identifiers</li> </ol> <p><code>sh    sudo blkid</code></p> <ol> <li>Edit <code>/etc/fstab</code> with UUID of disk and desired mount point</li> </ol> <p><code>sh    sudo nano /etc/fstab    ...    /dev/disk/by-uuid/&lt;UUID&gt; /mnt/&lt;MOUNTPOINT&gt; ext4 defaults 0 0</code></p> <ol> <li>Create mount location</li> </ol> <p><code>sh    sudo mkdir /mnt/&lt;MOUNTPOINT&gt;</code></p> <ol> <li>Mount the drive</li> </ol> <p><code>sh    sudo mount -a</code></p> <ol> <li>Check your work</li> </ol> <p><code>sh    sudo fdisk -l    ls /mnt/&lt;MOUNTPOINT&gt;</code></p>"},{"location":"notes/ubuntu_usb/","title":"Make bootable USB","text":"<p>Note: instructions will operate from current working directory and will extract to a directory called <code>iso2usb</code></p> <ol> <li>Download ISO Installer:</li> </ol> <p><code>sh    version=20.04.4    wget \"https://releases.ubuntu.com/20.04/ubuntu-$version-live-server-amd64.iso\"</code></p> <ol> <li>Extract ISO using xorriso and fix permissions</li> </ol> <p><code>sh    xorriso -osirrox on -indev \"./ubuntu-${version}-live-server-amd64.iso\" -extract / ./iso2usb    chmod -R +w ./iso2usb</code></p> <ol> <li>Set up local cloud-init (or see pxe setup to use webserver from pxe)</li> </ol> <p><code>sh    mkdir -p ./iso2usb/nocloud    touch ./iso2usb/nocloud/meta-data    cp user-data ./iso2usb/nocloud/user-data</code></p> <ol> <li>Update boot flags with cloud-init autoinstall:</li> </ol> <p><code>sh    # Should look similar to this: initrd=/casper/initrd quiet autoinstall ds=nocloud;s=/cdrom/nocloud/ ---    sed -i '' 's|---|autoinstall ds=nocloud\\\\\\;s=/cdrom/nocloud/ ---|g' ./iso2usb/boot/grub/grub.cfg    sed -i '' 's|---|autoinstall ds=nocloud;s=/cdrom/nocloud/ ---|g' ./iso2usb/isolinux/txt.cfg</code></p> <p>or, if we have a webserver hosting cloudinit images:</p> <p><code>sh    sed -i '' 's|---|autoinstall ds=\"nocloud-net;seedfrom=https://nas.domain.com:8081/pxeboot/cloud-init\"---|g' ./iso2usb/boot/grub/grub.cfg    sed -i '' 's|---|autoinstall ds=\"nocloud-net;seedfrom=https://nas.domain.com:8081/pxeboot/cloud-init\"---|g' ./iso2usb/isolinux/txt.cfg</code></p> <ol> <li>Disable mandatory md5 checksum on boot:</li> </ol> <p><code>sh    md5sum iso2usb/.disk/info &gt;! ./iso2usb/md5sum.txt    sed -i '' 's|iso2usb/|./|g' ./iso2usb/md5sum.txt</code></p> <p>[Optional] Regenerate md5:</p> <p>```sh</p> <ol> <li>Find/extract <code>isohdpfx.bin</code> from source iso</li> </ol> <p><code>sh    # note: extracts to current working directory    dd if=\"./iso/ubuntu-20.04.4-live-server-amd64.iso\" bs=1 count=432 of=\"isohdpfx.bin\"</code></p> <ol> <li>Create Install ISO from extracted dir:</li> </ol> <p><code>sh    xorriso -as mkisofs -r \\      -V Ubuntu\\ custom\\ amd64 \\      -o \"ubuntu-$version-live-server-amd64-autoinstall.iso\" \\      -J -l -b isolinux/isolinux.bin -c isolinux/boot.cat -no-emul-boot \\      -boot-load-size 4 -boot-info-table \\      -eltorito-alt-boot -e boot/grub/efi.img -no-emul-boot \\      -isohybrid-gpt-basdat -isohybrid-apm-hfsplus \\      -isohybrid-mbr ./isohdpfx.bin  \\      iso2usb/boot iso2usb</code></p> <ol> <li>Flash iso to usb drive with BalenaEtcher or equivalent</li> </ol>"},{"location":"notes/ubuntu_usb/#find-will-warn-file-system-loop-detected-and-return-non-zero-exit-status-on-the-ubuntu-symlink-to","title":"find will warn 'File system loop detected' and return non-zero exit status on the 'ubuntu' symlink to '.'","text":""},{"location":"notes/ubuntu_usb/#to-avoid-that-temporarily-move-it-out-of-the-way","title":"To avoid that, temporarily move it out of the way","text":"<p>mv iso2usb/ubuntu . cd ./iso2usb; find '!' -name \"md5sum.txt\" '!' -path \"./isolinux/*\" -follow -type f -exec \"$(which md5sum)\" {} \\; &gt; ../md5sum.txt) mv md5sum.txt ./iso2usb mv ubuntu ./iso2usb ```     </p>"},{"location":"notes/ubuntu_usb/#references","title":"References","text":"<ul> <li>https://nekodaemon.com/2022/01/16/Unattended-Ubuntu-20-04-Server-Offline-Installation/</li> <li>https://gist.github.com/s3rj1k/55b10cd20f31542046018fcce32f103e</li> <li>https://wiki.ubuntu.com/FoundationsTeam/AutomatedServerInstalls</li> <li>https://wiki.ubuntu.com/FoundationsTeam/AutomatedServerInstalls/ConfigReference</li> <li>https://cloudinit.readthedocs.io/en/latest/topics/datasources/nocloud.html</li> <li>https://discourse.ubuntu.com/t/please-test-autoinstalls-for-20-04/15250/53</li> <li>https://gist.github.com/dbkinghorn/c236aea31d76028b2b6ccdf6d3c6f07e</li> </ul>"}]}