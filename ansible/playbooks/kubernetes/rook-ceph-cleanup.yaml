---
- hosts:
    - controller
    - worker
  become: true
  gather_facts: true
  any_errors_fatal: true
  pre_tasks:
    - name: Verify
      pause:
        prompt: |
          Preparing to remove rook-ceph files local to nodes.
          This will destroy all data!
          Prior to this, confirm you have removed rook-ceph from the k8s cluster gitops.
          Please confirm (yes/no)
      register: confirm_remove

    - name: Confirm
      delegate_to: localhost
      run_once: true
      assert:
        that: confirm_remove.user_input | bool
        fail_msg: Exiting at user request ...
        success_msg: Continuing with cleanup ...

  tasks:
    # assumes removal of cluster from k8s
    - name: Remove /var/lib/rook
      ansible.builtin.file:
        state: absent
        path: /var/lib/rook

    - name: Zap the disk to a fresh, usable state (zap-all is important, b/c MBR has to be clean)
      ansible.builtin.command: sgdisk --zap-all "{{ rook_block_device }}" || true
    - name: Wipe a large portion of the beginning of the disk to remove more LVM metadata that may be present
      ansible.builtin.command: dd if=/dev/zero of="{{ rook_block_device }}" bs=1M count=100 oflag=direct,dsync
    - name: Wipe the block device with wipefs
      ansible.builtin.command: wipefs --all --force "{{ rook_block_device }}"
    - name: SSDs may be better cleaned with blkdiscard instead of dd
      ansible.builtin.command: blkdiscard "{{ rook_block_device }}"
    - name: Inform the OS of partition table changes
      ansible.builtin.command: partprobe "{{ rook_block_device }}"
    - name: Ceph can leave LVM and device mapper data that can lock the disks
      ansible.builtin.command: "{{ item }}"
      loop:
        - ls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove %
        - rm -rf /dev/ceph-*
        - rm -rf /dev/mapper/ceph--*
